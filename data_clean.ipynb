{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# one hot encoding\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "lemmatize = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"\"\"\n",
    "Chief Bola Ahmed Adekunle Tinubu GCFR (born 29 March 1952) is a Nigerian politician who is the 16th and current president of Nigeria.[1] He was the governor of Lagos State from 1999 to 2007; and senator for Lagos West in the Third Republic.\n",
    "\n",
    "Tinubu spent his early life in southwestern Nigeria and later moved to the United States where he studied accounting at Chicago State University. He returned to Nigeria in the early 1990s and was employed by Mobil Nigeria as an accountant, before entering politics as a Lagos West senatorial candidate in 1992 under the banner of the Social Democratic Party. After dictator Sani Abacha dissolved the Senate in 1993, Tinubu became an activist campaigning for the return of democracy as a part of the National Democratic Coalition movement.\n",
    "\n",
    "In the first post-transition Lagos State gubernatorial election, Tinubu won by a wide margin as a member of the Alliance for Democracy. Four years later, he won re-election to a second term. After leaving office in 2007, he played a key role in the formation of the All Progressives Congress in 2013. In 2023, he was elected president of Nigeria.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\ACER-\n",
      "[nltk_data]     PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\ACER-\n",
      "[nltk_data]     PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenization\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# sentences = nltk.sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for i in range(len(sentences)):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', sentences[i])\n",
    "    review=review.lower()\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to apply stemming\n",
    "for i in corpus:\n",
    "    words = nltk.word_tokenize(i)\n",
    "    for word in words:\n",
    "        if word not in set(stopwords.words('english')):\n",
    "            print(stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords.words('english')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paragraph = []\n",
    "\n",
    "# for i in range(len(sentences)):\n",
    "#     review = re.sub('[^a-zA-Z]', ' ', sentences[i])\n",
    "#     review = review.lower()\n",
    "#     review = review.split()\n",
    "#     review = [lemmatize.lemmatize(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "#     review = ' '.join(review) \n",
    "#     paragraph.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to apply lemmatizer\n",
    "\n",
    "for i in corpus:\n",
    "    words = nltk.word_tokenize(i)\n",
    "    for word in words:\n",
    "        if word not in set(stopwords.words('english')):\n",
    "            print(lemmatize.lemmatize(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bag of words\n",
    "\n",
    "cv = CountVectorizer(binary=True, ngram_range=(3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cv.fit_transform(corpus)\n",
    "cv.vocabulary_\n",
    "sorted(cv.vocabulary_.items())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TFIDF\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tv = TfidfVectorizer(ngram_range=(1,1), max_features=10)\n",
    "X = tv.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# READ CSV\n",
    "spam = pd.read_csv('spam_ham.csv', sep='\\t', names=['labels', 'message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# APPLY STEMMER\n",
    "corpus_stem = []\n",
    "\n",
    "for i in range(0, len(spam)):\n",
    "    review = re.sub('[^a-zA-Z0-9]', ' ', spam['message'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    review = [stemmer.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "    review = ' '.join(review) \n",
    "    corpus_stem.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bag of words\n",
    "cv = CountVectorizer(max_features=2500,binary=True)\n",
    "X = cv.fit_transform(corpus_stem).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.get_dummies(spam['labels'])\n",
    "y = y.iloc[:, 1].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0\n",
       "1       0\n",
       "2       1\n",
       "3       0\n",
       "4       0\n",
       "       ..\n",
       "5567    1\n",
       "5568    0\n",
       "5569    0\n",
       "5570    0\n",
       "5571    0\n",
       "Name: spam, Length: 5572, dtype: int32"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb = MultinomialNB().fit(X_train, y_train)\n",
    "# prediction\n",
    "y_pred = mnb.predict(X_test)\n",
    "accuracy_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification_report(y_pred, y_test)\n",
    "confusion_matrix(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFIDF\n",
    "\n",
    "tv = TfidfVectorizer(max_features=2500, ngram_range=(1,2))\n",
    "X = tv.fit_transform(corpus_stem).toarray()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb = MultinomialNB().fit(X_train, y_train)\n",
    "y_pred = mnb.predict(X_test)\n",
    "accuracy_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = RandomForestClassifier()\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "accuracy_score(y_pred, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gensim.downloader as api\n",
    "\n",
    "# wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "\n",
    "for i in range(0, len(spam)):\n",
    "    review = re.sub('[^a-zA-Z0-9]', ' ', spam['message'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    review = [lemmatize.lemmatize(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "    review = ' '.join(review) \n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['go jurong point crazy available bugis n great world la e buffet cine got amore wat',\n",
       " 'ok lar joking wif u oni',\n",
       " 'free entry 2 wkly comp win fa cup final tkts 21st may 2005 text fa 87121 receive entry question std txt rate c apply 08452810075over18',\n",
       " 'u dun say early hor u c already say',\n",
       " 'nah think go usf life around though']"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.get_dummies(y).iloc[:, 1].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from nltk import sent_tokenize\n",
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words = []\n",
    "# for i in corpus:\n",
    "#     sent_token = sent_tokenize(i)\n",
    "#     for i in sent_token:\n",
    "#         words.append(simple_preprocess(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "for i in corpus:\n",
    "    sent_token = sent_tokenize(i)\n",
    "    words.append(simple_preprocess(i))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5565"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5565"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(words, window=5,min_count=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "315"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.index_to_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5572"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.corpus_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_word2vec(doc):\n",
    "    # remove out-of-vocabulary words\n",
    "    # sent = [word for word in doc if word in model.mv.index_to_key]\n",
    "    # print(sent)\n",
    "    \n",
    "    return np.mean([model.wv[word] for word in words if word in model.wv.index_to_key], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5572 [00:00<?, ?it/s]c:\\Users\\ACER-PC\\Desktop\\Z\\BA_nltk\\.venv\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\ACER-PC\\Desktop\\Z\\BA_nltk\\.venv\\lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "100%|██████████| 5572/5572 [07:49<00:00, 11.87it/s]\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "\n",
    "for i in tqdm(range(len(words))):\n",
    "    X.append(avg_word2vec(words[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new =  np.array(X, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.0572608 ,  0.2736493 ,  0.02597114,  0.01283087,  0.01862401,\n",
       "       -0.38753533,  0.06568547,  0.51432174, -0.14124586, -0.12946813,\n",
       "       -0.10796515, -0.35931024, -0.0528903 ,  0.08321051,  0.07744681,\n",
       "       -0.24833035,  0.00578281, -0.3561922 , -0.01361859, -0.40739238,\n",
       "        0.15922056,  0.04068476,  0.11397326, -0.09942146, -0.11764296,\n",
       "        0.03255244, -0.24111311, -0.2693658 , -0.241899  ,  0.07553742,\n",
       "        0.31912678, -0.00396283,  0.11117529, -0.17404239, -0.09871449,\n",
       "        0.2199322 , -0.06809594, -0.23408274, -0.14690335, -0.49812046,\n",
       "       -0.00717556, -0.27797118, -0.05698733,  0.03035105,  0.21321188,\n",
       "       -0.18032382, -0.17816865,  0.0175348 ,  0.10231765,  0.1819616 ,\n",
       "        0.12994821, -0.20816559, -0.03378843, -0.04842072, -0.18076712,\n",
       "        0.11473767,  0.13337092,  0.03621004, -0.24753475, -0.02266791,\n",
       "        0.02906995,  0.13778256, -0.11427843,  0.04078652, -0.36477742,\n",
       "        0.32023242,  0.11158089,  0.21156524, -0.33866885,  0.35055384,\n",
       "       -0.18943457,  0.1342479 ,  0.31816602, -0.07681277,  0.25222653,\n",
       "        0.1915256 , -0.04006084, -0.03074008, -0.21229695,  0.14412732,\n",
       "       -0.12195331, -0.01582775, -0.23878054,  0.29332414, -0.02452704,\n",
       "        0.0094473 , -0.03818645,  0.2900832 ,  0.33815405,  0.17110066,\n",
       "        0.28716537,  0.08985431,  0.01727141,  0.08772993,  0.39754856,\n",
       "        0.25061768,  0.18387683, -0.2365766 ,  0.06216813, -0.0880762 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TENSORFLOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.13.0\n"
     ]
    }
   ],
   "source": [
    "# tensorflow version\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "jet = ['go jurong point crazy available bugis n great',\n",
    " 'ok lar joking wif u oni',\n",
    " 'free entry wkly comp win fa cup final tkts 21st may  ',\n",
    " 'u dun say early hor u c already say',\n",
    " 'nah think go usf life around though']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  vocabulary size\n",
    "voc_size = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = [one_hot(words, voc_size) for words in jet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8101, 18232, 16013, 9412, 8453, 1284, 9778, 1843], [1022, 9959, 10345, 3091, 11908, 962], [3942, 17683, 12474, 666, 11071, 3125, 271, 7263, 17484, 18680, 15243], [11908, 2133, 2156, 14758, 8484, 11908, 5829, 10351, 2156], [369, 12479, 8101, 15707, 13015, 17367, 9865]]\n"
     ]
    }
   ],
   "source": [
    "print(ohe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0     0     0     0     0     0  8101 18232 16013  9412  8453  1284\n",
      "   9778  1843]\n",
      " [    0     0     0     0     0     0     0     0  1022  9959 10345  3091\n",
      "  11908   962]\n",
      " [    0     0  3942 17683 12474   666 11071  3125   271  7263 17484 18680\n",
      "  15243  5718]\n",
      " [    0     0     0     0     0 11908  2133  2156 14758  8484 11908  5829\n",
      "  10351  2156]\n",
      " [    0     0     0     0     0     0     0   369 12479  8101 15707 13015\n",
      "  17367  9865]]\n"
     ]
    }
   ],
   "source": [
    "# pre padding\n",
    "max_len = 14\n",
    "embedded_doc = pad_sequences(ohe, padding='pre', maxlen=max_len)\n",
    "print(embedded_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimension\n",
    "dim = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(voc_size, dim, input_length=max_len))\n",
    "model.compile('adam', 'mse')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 14, 10)            200000    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 200000 (781.25 KB)\n",
      "Trainable params: 200000 (781.25 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 917ms/step\n",
      "[[ 1.37027390e-02  7.50728697e-03 -1.91537291e-03  2.25561298e-02\n",
      "  -4.65618260e-02  7.38807768e-03 -3.20731774e-02  2.13506483e-02\n",
      "  -3.37979421e-02 -1.23065338e-02]\n",
      " [ 1.37027390e-02  7.50728697e-03 -1.91537291e-03  2.25561298e-02\n",
      "  -4.65618260e-02  7.38807768e-03 -3.20731774e-02  2.13506483e-02\n",
      "  -3.37979421e-02 -1.23065338e-02]\n",
      " [-1.84089169e-02  3.16628069e-03  4.81788255e-02  1.79197080e-02\n",
      "  -2.31021885e-02  3.51349972e-02 -5.32878563e-03  2.62005590e-02\n",
      "  -4.69735973e-02 -1.19806640e-02]\n",
      " [-1.44460201e-02 -2.73765083e-02 -1.52546056e-02  4.76735272e-02\n",
      "   4.69791889e-03 -1.02108940e-02  4.84446920e-02  1.51384510e-02\n",
      "  -4.74895947e-02 -4.49028127e-02]\n",
      " [ 1.84535980e-03 -4.27164882e-03  3.01960856e-03 -9.81173664e-03\n",
      "  -5.98663092e-03  4.48032655e-02 -4.50769812e-03 -3.84693518e-02\n",
      "  -1.52222142e-02  8.34927708e-03]\n",
      " [-7.14976713e-03 -4.59748283e-02  2.72828974e-02  3.95735838e-02\n",
      "  -3.24606076e-02 -4.51539829e-03  6.97492436e-03 -2.54309531e-02\n",
      "  -4.90478873e-02 -5.54718077e-04]\n",
      " [-1.07235909e-02 -3.06231510e-02 -2.72406582e-02 -1.23188980e-02\n",
      "   1.96511783e-02 -3.38841453e-02  1.24299750e-02 -2.12641489e-02\n",
      "  -1.31221637e-02  4.89315726e-02]\n",
      " [-4.62344661e-02 -2.70239245e-02 -2.97098402e-02  4.87943180e-02\n",
      "  -4.50597294e-02  1.85152777e-02 -3.48824635e-02  1.28082670e-02\n",
      "  -1.52549259e-02 -3.03954612e-02]\n",
      " [ 9.25624371e-03 -2.85994168e-02  1.89530514e-02 -1.11559629e-02\n",
      "  -3.97615321e-02 -2.54115108e-02 -4.70452309e-02 -9.09693167e-03\n",
      "  -1.25667825e-02 -1.70555711e-02]\n",
      " [ 3.01026739e-02 -3.99435647e-02 -1.63478144e-02  8.92453268e-03\n",
      "   1.43376738e-03 -5.75476885e-03 -5.01976162e-03  5.76972961e-04\n",
      "  -2.68961918e-02 -7.16090202e-04]\n",
      " [ 4.33267094e-02  2.71226503e-02  7.20247626e-05 -1.74983256e-02\n",
      "  -6.76716492e-03  4.98418882e-03 -2.36848835e-02  5.06527349e-03\n",
      "   2.98285149e-02  1.22733936e-02]\n",
      " [ 2.67605521e-02  3.70993949e-02 -3.29539329e-02 -4.54017632e-02\n",
      "  -3.49896550e-02 -2.90844794e-02 -1.94293614e-02 -3.33062783e-02\n",
      "   2.10646726e-02 -7.16469437e-03]\n",
      " [-3.24634686e-02 -2.79428363e-02  3.04790474e-02  2.96954997e-02\n",
      "   3.25673819e-03 -3.51321809e-02  1.89401992e-02 -1.45417675e-02\n",
      "  -4.88847978e-02  1.66784972e-04]\n",
      " [-2.49472260e-02  1.08314976e-02 -4.30480614e-02 -4.14740071e-02\n",
      "  -2.89240833e-02  1.52834691e-02  1.01089701e-02  1.55155547e-02\n",
      "  -3.25496197e-02  1.44465603e-02]]\n"
     ]
    }
   ],
   "source": [
    "print(model.predict(embedded_doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     0,  3942, 17683, 12474,   666, 11071,  3125,   271,\n",
       "        7263, 17484, 18680, 15243,  5718])"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_doc[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(embedded_doc)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = [\n",
    "    'the edit is nice',\n",
    "    'the format is intact',\n",
    "    'an excellent invoice',\n",
    "    'a graphic designer'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[682, 238, 642, 817], [682, 161, 642, 851], [124, 765, 443], [327, 493, 967]]"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voca_size = 1000\n",
    "dimension = 8\n",
    "\n",
    "encod = [one_hot(word, voca_size) for word in sentence]\n",
    "encod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[682 238 642 817   0   0]\n",
      " [682 161 642 851   0   0]\n",
      " [124 765 443   0   0   0]\n",
      " [327 493 967   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "sent_len = 6\n",
    "\n",
    "embededd = pad_sequences(encod, padding='post', maxlen=sent_len)\n",
    "print(embededd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(voca_size, dimension,input_length=sent_len))\n",
    "model.compile('adam', 'mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 6, 8)              8000      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8000 (31.25 KB)\n",
      "Trainable params: 8000 (31.25 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 234ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.01730249, -0.02905182, -0.04203447,  0.04622216,  0.03759099,\n",
       "        -0.03445238,  0.04127964, -0.03031659],\n",
       "       [ 0.00340147, -0.02760332, -0.03287043, -0.0456552 ,  0.01825352,\n",
       "         0.01187756,  0.03332193, -0.03357146],\n",
       "       [ 0.04581827,  0.00497971,  0.01739368,  0.01423415,  0.0204906 ,\n",
       "         0.03918267, -0.00315655,  0.0364134 ],\n",
       "       [-0.00851937,  0.02483142, -0.03257564,  0.04078094,  0.01232474,\n",
       "         0.03391645, -0.0161798 , -0.01858059],\n",
       "       [-0.02053428,  0.0177946 , -0.01511121, -0.01588248,  0.00129557,\n",
       "        -0.01895362,  0.02527023,  0.04015435],\n",
       "       [-0.02053428,  0.0177946 , -0.01511121, -0.01588248,  0.00129557,\n",
       "        -0.01895362,  0.02527023,  0.04015435]], dtype=float32)"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(embededd)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
